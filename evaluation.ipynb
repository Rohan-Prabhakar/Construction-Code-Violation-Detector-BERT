{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14764110,"sourceType":"datasetVersion","datasetId":9437034}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport requests\nimport pandas as pd\nfrom tqdm import tqdm\n\nos.makedirs('data/raw', exist_ok=True)\n\nECB_API = \"https://data.cityofnewyork.us/resource/6bgk-3dad.json\"\nSAFETY_API = \"https://data.cityofnewyork.us/resource/855j-jady.json\"\nHPD_API = \"https://data.cityofnewyork.us/resource/wvxf-dwi5.json\"\n\ndef fetch_dataset(api_url, name, total=200000, batch=50000):\n    records = []\n    for offset in range(0, total, batch):\n        print(f\"[{name}] Fetching {offset:,}-{offset+batch:,}...\")\n        params = {\"$limit\": batch, \"$offset\": offset, \"$order\": \":id\"}\n        resp = requests.get(api_url, params=params)\n        if resp.status_code != 200:\n            print(f\"  Error: {resp.status_code}\")\n            break\n        data = resp.json()\n        if not data:\n            break\n        records.extend(data)\n    df = pd.DataFrame(records)\n    print(f\"[{name}] Total: {len(df):,} records\\n\")\n    return df\n\nprint(\"=\" * 60)\nprint(\"DOWNLOADING NYC BUILDING VIOLATION DATASETS\")\nprint(\"=\" * 60)\n\n# 1. DOB ECB Violations (primary dataset)\necb_path = \"data/raw/ecb_violations.csv\"\nif not os.path.exists(ecb_path):\n    ecb_df = fetch_dataset(ECB_API, \"ECB Violations\", total=300000)\n    ecb_df.to_csv(ecb_path, index=False)\n    print(f\"Saved: {ecb_path}\")\nelse:\n    ecb_df = pd.read_csv(ecb_path)\n    print(f\"Cached: {ecb_path} ({len(ecb_df):,} rows)\")\n\n# 2. DOB Safety Violations (supplementary)\nsafety_path = \"data/raw/safety_violations.csv\"\nif not os.path.exists(safety_path):\n    safety_df = fetch_dataset(SAFETY_API, \"Safety Violations\", total=100000)\n    safety_df.to_csv(safety_path, index=False)\n    print(f\"Saved: {safety_path}\")\nelse:\n    safety_df = pd.read_csv(safety_path)\n    print(f\"Cached: {safety_path} ({len(safety_df):,} rows)\")\n\n# 3. HPD Housing Violations (supplementary)\nhpd_path = \"data/raw/hpd_violations.csv\"\nif not os.path.exists(hpd_path):\n    hpd_df = fetch_dataset(HPD_API, \"HPD Violations\", total=100000)\n    hpd_df.to_csv(hpd_path, index=False)\n    print(f\"Saved: {hpd_path}\")\nelse:\n    hpd_df = pd.read_csv(hpd_path)\n    print(f\"Cached: {hpd_path} ({len(hpd_df):,} rows)\")\n\n\nfor name, df in [(\"ECB Violations\", ecb_df), (\"Safety Violations\", safety_df), (\"HPD Violations\", hpd_df)]:\n    print(f\"\\n{name}:\")\n    print(f\"  Rows: {len(df):,}\")\n    print(f\"  Columns: {len(df.columns)}\")\n    print(f\"  Columns: {list(df.columns)[:10]}...\")\n    print(f\"  Missing: {df.isnull().sum().sum():,} total null values\")\n\nprint(\"\\n\\nECB Violations - Key Columns Preview:\")\npreview_cols = [c for c in ['violation_type', 'violation_description', 'severity',\n                            'section_of_law1_description', 'infraction_codes'] if c in ecb_df.columns]\nprint(ecb_df[preview_cols].head(10).to_string())\n\nprint(\"\\n\\nECB Violation Type Distribution:\")\nif 'violation_type' in ecb_df.columns:\n    print(ecb_df['violation_type'].value_counts().head(15))\n\nprint(\"\\nECB Severity Distribution:\")\nif 'severity' in ecb_df.columns:\n    print(ecb_df['severity'].value_counts())\n\nprint(\"\\n✓ All datasets downloaded. Ready for 01_dataset_preparation.py\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:44:00.891749Z","iopub.execute_input":"2026-02-07T15:44:00.892306Z","iopub.status.idle":"2026-02-07T15:45:11.487498Z","shell.execute_reply.started":"2026-02-07T15:44:00.892277Z","shell.execute_reply":"2026-02-07T15:45:11.486699Z"}},"outputs":[{"name":"stdout","text":"============================================================\nDOWNLOADING NYC BUILDING VIOLATION DATASETS\n============================================================\n[ECB Violations] Fetching 0-50,000...\n[ECB Violations] Fetching 50,000-100,000...\n[ECB Violations] Fetching 100,000-150,000...\n[ECB Violations] Fetching 150,000-200,000...\n[ECB Violations] Fetching 200,000-250,000...\n[ECB Violations] Fetching 250,000-300,000...\n[ECB Violations] Total: 300,000 records\n\nSaved: data/raw/ecb_violations.csv\n[Safety Violations] Fetching 0-50,000...\n[Safety Violations] Fetching 50,000-100,000...\n[Safety Violations] Total: 100,000 records\n\nSaved: data/raw/safety_violations.csv\n[HPD Violations] Fetching 0-50,000...\n[HPD Violations] Fetching 50,000-100,000...\n[HPD Violations] Total: 100,000 records\n\nSaved: data/raw/hpd_violations.csv\n\nECB Violations:\n  Rows: 300,000\n  Columns: 42\n  Columns: ['isn_dob_bis_extract', 'ecb_violation_number', 'ecb_violation_status', 'dob_violation_number', 'bin', 'boro', 'block', 'lot', 'hearing_date', 'hearing_time']...\n  Missing: 4,471,620 total null values\n\nSafety Violations:\n  Rows: 100,000\n  Columns: 24\n  Columns: ['bin', 'violation_issue_date', 'violation_number', 'violation_type', 'violation_remarks', 'violation_status', 'device_type', 'borough', 'block', 'lot']...\n  Missing: 139,532 total null values\n\nHPD Violations:\n  Rows: 100,000\n  Columns: 41\n  Columns: ['violationid', 'buildingid', 'registrationid', 'boroid', 'boro', 'housenumber', 'lowhousenumber', 'highhousenumber', 'streetname', 'streetcode']...\n  Missing: 284,185 total null values\n\n\nECB Violations - Key Columns Preview:\n  violation_type                                                                                                                                                                                               violation_description   severity\n0   Construction  FAILURE TO PROVIDE AT LEAST TWO MEANS OF EGRESS FROM OR SPACE WHERE REQUIRED. NOTE: DEPARTMENT OF BUILDINGS RECORDS INDICATED RESIDENCE TO  BE LEGALLY APPROVE AS 2 FAMILY NOW ON THE 2ND AND 3RD FLR THEY HAVE 10  Hazardous\n1   Construction   FAILURE TO MAINTAIN BUILDING HAZARDOUS. NOTE: AT THE CELLAR LEVEL THE BOILER ROOM CEILING IS EXPOSE TO WOOD JOINS AND ABOVE THE BOILER ROOM IS A BEDROOM WITH BED. THE BUILDINGS IS LEGALLY APPROVED FOR 2 FAMILY  Hazardous\n2    Site Safety      FAILURE TO SAFEGUARD PUBLIC & PROPERTY AFFECTED BY DEMO OPERATIONS. UPON INSP AT 325 20TH ST 2 STORIES BLDG IS DOWN, SITE IS GRADED, CONTRACTOR IS ASKING FOR SIGN OFF ON DEMO, ADJACENT PROPERTY 327 20TH ST.  Hazardous\n3    Site Safety      MECH DEMO W/O PERMIT. UPON INSPECTION AT 189 3RD AVE DEMO CREW ON SITEREMOVING 1 STORIES BUILDING REAR WITH MACHINES NO PERMIT WAS ISSUED FOR MECHANICAL DEMOLITION FROM DOB FOR THIS ACTION. REMEDY: STOP ALL  Hazardous\n4    Site Safety    MECH DEMO W/O PERMIT UPON INSP AT 294 BUTLER ST DEMO CREW ON SITE USING MACHINE TO REMOVE BUILDING REAR NO PERMIT WAS ISSUED  FOR MECHANICALDEMOLITION FROM DOB FOR THIS OPERATIONS. REMEDY: STOP ALL MECHANICAL  Hazardous\n5   Construction  FAILURE TO PROVIDE SITE SAFETY MGR ON SITE. 3 STORY BLDG UNDER DEMOLITION. SITE ACTIVE WORKERS ON SITE. SITE DESIGNATED AS SITE SAFETY AS PER BKLYN BORO COMMISSIONER. UPON ARRIVAL ON SITE 1010 AM NO SITE SAFETY  CLASS - 1\n6   Construction  FAILURE TO MAINTAIN BLDG WALL OR APPURTENANCES NOTE AT FRONT 3RD FL AND WEST SIDE METAL SHEATING IS LOOSE AND PEELING AWAY ALSO NOT THAT AT VARIOUS LOCATIONS ASPHALT SHINGLES ARE DETERIORATED OR MISSING REMEDY:  CLASS - 2\n7   Construction     FAILURE TO MAINTAIN ADEQUATE HOUSEKEEPING DEFECTS NOTED AT FRONT YARD SETBACK FOR VACANT OPEN AND UNGUARDED BLDG FOUND WITH THE STORAGE OF NUMEROUS BACK PLASTIC BAGS FULL OF DEBRIS AND ALSO NOTED THE STORAGE  CLASS - 2\n8   Construction         FAILURE TO MAINTAIN BLDG WALL AND OR APPURTENANCES NOTED BLDG FOUND TO BE VACANT OPEN AND UNGUARDED IN THAT IT IS OPENED AT FRONT FACADE AND REAR WINDOWS LEAVING IT OPENED AND EXPOSED TO THE ELEMENTS FOR  CLASS - 1\n9       Plumbing  USE OR INSTALLATION OF PLUMBING MATERIALS OR EQUIPMENT WHICH DO NOT COMPLY WITH RS-16. ON 1ST FLR 2 COMPARTMENT SINK DRAIN AND 3' DRAIN LINE IN CELL INSTALLED WITH PVC PIPE (NOT ALLOWED FOR COMMERCIAL SPACE) IN  Hazardous\n\n\nECB Violation Type Distribution:\nviolation_type\nConstruction           156931\nElevators               52893\nUnknown                 40800\nBoilers                 14402\nLocal Law                7960\nSigns                    6699\nPublic Assembly          5783\nSite Safety              3713\nZoning                   3169\nCranes and Derricks      3024\nPlumbing                 2839\nQuality of Life          1494\nHPD                       278\nAdministrative             15\nName: count, dtype: int64\n\nECB Severity Distribution:\nseverity\nCLASS - 2        100380\nNon-Hazardous     93596\nCLASS - 1         71057\nHazardous         17151\nCLASS - 3         14983\nUnknown            2833\nName: count, dtype: int64\n\n✓ All datasets downloaded. Ready for 01_dataset_preparation.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\nfor d in ['data/processed', 'data/splits', 'figures']:\n    os.makedirs(d, exist_ok=True)\n\nprint(\"=\" * 70)\nprint(\"DATASET PREPARATION FOR BUILDING CODE VIOLATION DETECTION\")\nprint(\"=\" * 70)\n\n\n# ==============================================================================\n# SECTION 1: DATASET SELECTION (3 Points)\n# ==============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SECTION 1: DATASET SELECTION\")\nprint(\"=\" * 70)\n\nraw_df = pd.read_csv(\"data/raw/ecb_violations.csv\")\nprint(f\"\\n✓ Loaded raw dataset: {len(raw_df):,} records, {len(raw_df.columns)} columns\")\n\nprint(f\"\\n--- Raw Dataset Overview ---\")\nprint(f\"Shape: {raw_df.shape}\")\nprint(f\"Date range: {raw_df['violation_date'].min()} to {raw_df['violation_date'].max()}\"\n      if 'violation_date' in raw_df.columns else \"\")\n\nprint(f\"\\nRaw Violation Type Distribution:\")\nprint(raw_df['violation_type'].value_counts())\n\nprint(f\"\\nRaw Severity Distribution:\")\nprint(raw_df['severity'].value_counts())\n\n# Visualize raw distributions\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nraw_df['violation_type'].value_counts().plot(kind='barh', ax=axes[0], color='steelblue')\naxes[0].set_title('Raw Violation Type Distribution')\naxes[0].set_xlabel('Count')\n\nraw_df['severity'].value_counts().plot(kind='barh', ax=axes[1], color='coral')\naxes[1].set_title('Raw Severity Distribution')\naxes[1].set_xlabel('Count')\n\nplt.tight_layout()\nplt.savefig('figures/01_raw_distributions.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved: figures/01_raw_distributions.png\")\n\n\n# ==============================================================================\n# SECTION 2: PREPROCESSING AND DATA CLEANING (3 Points)\n# ==============================================================================\n\"\"\"\nPREPROCESSING PIPELINE:\n1. Drop records with missing violation_description or violation_type\n2. Clean and normalize violation descriptions (free text)\n3. Map 14 raw violation types → 8 standardized categories\n4. Map 6 raw severity codes → 3 classes (Hazardous / Moderate / Minor)\n5. Remove duplicates and validate data integrity\n6. Analyze and document class imbalance\n\nCLEANING OPERATIONS ON violation_description:\n- Replace specific addresses with <ADDR> token\n- Replace dollar amounts with <AMOUNT> token\n- Replace dates with <DATE> token\n- Replace permit/BIN numbers with <NUM> token\n- Replace floor/apartment references with <FLOOR> token\n- Normalize building code references\n- Normalize whitespace and casing\n- Remove non-printable characters\n\"\"\"\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SECTION 2: PREPROCESSING AND DATA CLEANING\")\nprint(\"=\" * 70)\n\n# ---------------------------------------------------------------------------\n# 2.1 HANDLE MISSING VALUES\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 2.1 Handling Missing Values ---\")\nprint(f\"Before: {len(raw_df):,} records\")\n\nmissing_before = raw_df[['violation_type', 'violation_description', 'severity']].isnull().sum()\nprint(f\"\\nMissing values in key columns:\")\nprint(missing_before)\n\ndf = raw_df.dropna(subset=['violation_description', 'violation_type', 'severity']).copy()\ndf = df[df['violation_description'].str.strip().str.len() > 10]\n\nprint(f\"After dropping nulls + short descriptions: {len(df):,} records\")\nprint(f\"Dropped: {len(raw_df) - len(df):,} records ({(len(raw_df)-len(df))/len(raw_df)*100:.1f}%)\")\n\n\n# ---------------------------------------------------------------------------\n# 2.2 VIOLATION DESCRIPTION CLEANING\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 2.2 Cleaning Violation Descriptions ---\")\n\nclass ViolationPreprocessor:\n    \"\"\"\n    Preprocessor for building code violation descriptions.\n\n    Transforms raw inspector-written descriptions into clean,\n    normalized text suitable for transformer-based models.\n    \"\"\"\n\n    def __init__(self):\n        self.cleaning_patterns = [\n            (r'\\b\\d{1,5}\\s+[\\w\\s]+(STREET|ST|AVENUE|AVE|ROAD|RD|BLVD|DRIVE|DR|PLACE|PL|LANE|LN)\\b', '<ADDR>'),\n            (r'\\$[\\d,]+\\.?\\d*', '<AMOUNT>'),\n            (r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', '<DATE>'),\n            (r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>'),\n            (r'\\bBIN\\s*#?\\s*\\d+\\b', '<BIN_NUM>'),\n            (r'\\bJOB\\s*#?\\s*\\d+\\b', '<JOB_NUM>'),\n            (r'\\bAPPLICATION\\s*#?\\s*\\d+\\b', '<APP_NUM>'),\n            (r'\\bPERMIT\\s*#?\\s*\\d+\\b', '<PERMIT_NUM>'),\n            (r'\\b\\d+(ST|ND|RD|TH)\\s*(FLOOR|FLR|FL)\\b', '<FLOOR>'),\n            (r'\\b(CELLAR|BASEMENT|ROOF|PENTHOUSE)\\b', '<FLOOR>'),\n            (r'\\b(1ST|2ND|3RD|\\d+TH)\\s*(STORY|STORIES)\\b', '<STORIES>'),\n            (r'\\bBC\\s*\\d+[\\.\\d]*\\s*;\\s*\\d+-\\d+', '<BLDG_CODE>'),\n            (r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', '<IP>'),\n            (r'(?<=[^A-Z\\d])\\d{5,}(?=[^A-Z\\d]|$)', '<NUM>'),\n        ]\n\n        self.stats = {'total': 0, 'cleaned': 0, 'empty_after': 0}\n\n    def clean_description(self, text):\n        self.stats['total'] += 1\n\n        if pd.isna(text) or not isinstance(text, str):\n            self.stats['empty_after'] += 1\n            return \"\"\n\n        cleaned = text.upper().strip()\n\n        for pattern, replacement in self.cleaning_patterns:\n            cleaned = re.sub(pattern, replacement, cleaned)\n\n        cleaned = re.sub(r'[^\\w\\s<>.,;:\\-/()&]', ' ', cleaned)\n        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n        cleaned = ''.join(c for c in cleaned if c.isprintable() or c == ' ')\n\n        if len(cleaned) < 10:\n            self.stats['empty_after'] += 1\n            return \"\"\n\n        self.stats['cleaned'] += 1\n        return cleaned\n\n    def print_stats(self):\n        print(f\"\\nCleaning Statistics:\")\n        print(f\"  Total processed: {self.stats['total']:,}\")\n        print(f\"  Successfully cleaned: {self.stats['cleaned']:,}\")\n        print(f\"  Empty after cleaning: {self.stats['empty_after']:,}\")\n\n\npreprocessor = ViolationPreprocessor()\n\nprint(\"Cleaning violation descriptions...\")\ndf['clean_description'] = df['violation_description'].apply(preprocessor.clean_description)\npreprocessor.print_stats()\n\ndf = df[df['clean_description'].str.len() > 10].copy()\nprint(f\"Records after cleaning: {len(df):,}\")\n\nprint(\"\\n--- Before vs After Cleaning Examples ---\")\nfor i in range(min(5, len(df))):\n    row = df.iloc[i]\n    print(f\"\\nExample {i+1}:\")\n    print(f\"  BEFORE: {str(row['violation_description'])[:120]}...\")\n    print(f\"  AFTER:  {str(row['clean_description'])[:120]}...\")\n\n\n# ---------------------------------------------------------------------------\n# 2.3 LABEL MAPPING - VIOLATION CATEGORY\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 2.3 Label Mapping: Violation Categories ---\")\n\nCATEGORY_MAP = {\n    'Construction':        'Construction',\n    'Elevators':           'Elevators',\n    'Boilers':             'Mechanical',\n    'Local Law':           'Regulatory',\n    'Signs':               'Regulatory',\n    'Public Assembly':     'Regulatory',\n    'Site Safety':         'Site Safety',\n    'Zoning':              'Zoning',\n    'Cranes and Derricks': 'Site Safety',\n    'Plumbing':            'Plumbing',\n    'Quality of Life':     'Quality of Life',\n    'HPD':                 'Construction',\n    'Administrative':      'Regulatory',\n    'Unknown':             None,\n}\n\ndf['category'] = df['violation_type'].map(CATEGORY_MAP)\ndf = df.dropna(subset=['category']).copy()\nprint(f\"Records after removing 'Unknown' type: {len(df):,}\")\n\nprint(f\"\\nMapped Category Distribution:\")\nprint(df['category'].value_counts())\nprint(f\"\\nTotal categories: {df['category'].nunique()}\")\n\n\n# ---------------------------------------------------------------------------\n# 2.4 LABEL MAPPING - SEVERITY\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 2.4 Label Mapping: Severity ---\")\n\nSEVERITY_MAP = {\n    'Hazardous':      'HIGH',\n    'CLASS - 1':      'HIGH',\n    'CLASS - 2':      'MEDIUM',\n    'CLASS - 3':      'LOW',\n    'Non-Hazardous':  'LOW',\n    'Unknown':        None,\n}\n\ndf['severity_label'] = df['severity'].map(SEVERITY_MAP)\ndf = df.dropna(subset=['severity_label']).copy()\nprint(f\"Records after removing 'Unknown' severity: {len(df):,}\")\n\nprint(f\"\\nMapped Severity Distribution:\")\nprint(df['severity_label'].value_counts())\n\n\n# ---------------------------------------------------------------------------\n# 2.5 ENCODE LABELS\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 2.5 Encoding Labels ---\")\n\ncategory_labels = sorted(df['category'].unique())\nseverity_labels = ['LOW', 'MEDIUM', 'HIGH']\n\ncat2id = {c: i for i, c in enumerate(category_labels)}\nid2cat = {i: c for c, i in cat2id.items()}\nsev2id = {s: i for i, s in enumerate(severity_labels)}\nid2sev = {i: s for s, i in sev2id.items()}\n\ndf['category_id'] = df['category'].map(cat2id)\ndf['severity_id'] = df['severity_label'].map(sev2id)\n\nprint(f\"\\nCategory Label Encoding:\")\nfor cat, idx in cat2id.items():\n    count = (df['category_id'] == idx).sum()\n    print(f\"  {idx}: {cat:<20s} ({count:,} samples)\")\n\nprint(f\"\\nSeverity Label Encoding:\")\nfor sev, idx in sev2id.items():\n    count = (df['severity_id'] == idx).sum()\n    print(f\"  {idx}: {sev:<10s} ({count:,} samples)\")\n\nlabel_maps = {'cat2id': cat2id, 'id2cat': id2cat, 'sev2id': sev2id, 'id2sev': id2sev}\nimport json\nwith open('data/processed/label_maps.json', 'w') as f:\n    json.dump(label_maps, f, indent=2)\nprint(\"✓ Saved: data/processed/label_maps.json\")\n\n\n# ---------------------------------------------------------------------------\n# 2.6 REMOVE DUPLICATES + FINAL VALIDATION\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 2.6 Deduplication & Validation ---\")\nprint(f\"Before dedup: {len(df):,}\")\n\ndf = df.drop_duplicates(subset=['clean_description', 'category_id', 'severity_id']).copy()\nprint(f\"After dedup: {len(df):,}\")\n\nassert df['clean_description'].isnull().sum() == 0, \"Null descriptions found!\"\nassert df['category_id'].isnull().sum() == 0, \"Null category labels found!\"\nassert df['severity_id'].isnull().sum() == 0, \"Null severity labels found!\"\nassert df['clean_description'].str.len().min() > 10, \"Short descriptions found!\"\nprint(\"✓ All validation checks passed\")\n\n\n# ---------------------------------------------------------------------------\n# 2.7 CLASS IMBALANCE ANALYSIS\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 2.7 Class Imbalance Analysis ---\")\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\ncat_counts = df['category'].value_counts()\ncat_counts.plot(kind='barh', ax=axes[0], color='steelblue')\naxes[0].set_title('Violation Category Distribution (Cleaned)')\naxes[0].set_xlabel('Count')\nfor i, v in enumerate(cat_counts.values):\n    axes[0].text(v + 500, i, f'{v:,}', va='center', fontsize=9)\n\nsev_counts = df['severity_label'].value_counts()\nsev_counts.plot(kind='barh', ax=axes[1], color='coral')\naxes[1].set_title('Severity Distribution (Cleaned)')\naxes[1].set_xlabel('Count')\nfor i, v in enumerate(sev_counts.values):\n    axes[1].text(v + 500, i, f'{v:,}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.savefig('figures/02_cleaned_distributions.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved: figures/02_cleaned_distributions.png\")\n\nimbalance_ratio_cat = cat_counts.max() / cat_counts.min()\nimbalance_ratio_sev = sev_counts.max() / sev_counts.min()\nprint(f\"\\nCategory imbalance ratio (max/min): {imbalance_ratio_cat:.1f}x\")\nprint(f\"Severity imbalance ratio (max/min): {imbalance_ratio_sev:.1f}x\")\nprint(\"→ Will use class weights during training to handle imbalance\")\n\nlaw_col = [c for c in df.columns if 'section_of_law' in c.lower() or 'sectionoflaw' in c.lower() or 'infraction' in c.lower()]\nkeep_cols = ['clean_description', 'category', 'category_id',\n             'severity_label', 'severity_id', 'violation_type', 'severity'] + law_col\nkeep_cols = [c for c in keep_cols if c in df.columns]\ndf_processed = df[keep_cols].copy()\ndf_processed = df_processed.reset_index(drop=True)\ndf_processed.to_csv('data/processed/violations_cleaned.csv', index=False)\nprint(f\"\\n✓ Saved: data/processed/violations_cleaned.csv ({len(df_processed):,} records)\")\n\n\n# ==============================================================================\n# SECTION 3: TRAIN / VALIDATION / TEST SPLITTING (3 Points)\n# ==============================================================================\n\"\"\"\nSPLITTING STRATEGY:\n- 80% Train / 10% Validation / 10% Test\n- Stratified by BOTH category_id AND severity_id to maintain distributions\n- Combined stratification key ensures all label combinations are represented\n- Reproducible via RANDOM_SEED = 42\n\"\"\"\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SECTION 3: TRAIN / VALIDATION / TEST SPLITTING\")\nprint(\"=\" * 70)\n\ndf_processed['strat_key'] = df_processed['category_id'].astype(str) + \"_\" + df_processed['severity_id'].astype(str)\n\nmin_class_count = df_processed['strat_key'].value_counts().min()\nprint(f\"Smallest stratification group: {min_class_count} samples\")\n\nif min_class_count < 3:\n    rare_keys = df_processed['strat_key'].value_counts()[df_processed['strat_key'].value_counts() < 3].index\n    print(f\"Removing {len(rare_keys)} rare combinations with < 3 samples\")\n    df_processed = df_processed[~df_processed['strat_key'].isin(rare_keys)].copy()\n    print(f\"Records after removing rare combos: {len(df_processed):,}\")\n\ntrain_df, temp_df = train_test_split(\n    df_processed, test_size=0.2, random_state=RANDOM_SEED,\n    stratify=df_processed['strat_key']\n)\n\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.5, random_state=RANDOM_SEED,\n    stratify=temp_df['strat_key']\n)\n\nprint(f\"\\n--- Split Sizes ---\")\nprint(f\"Train:      {len(train_df):>8,} ({len(train_df)/len(df_processed)*100:.1f}%)\")\nprint(f\"Validation: {len(val_df):>8,} ({len(val_df)/len(df_processed)*100:.1f}%)\")\nprint(f\"Test:       {len(test_df):>8,} ({len(test_df)/len(df_processed)*100:.1f}%)\")\nprint(f\"Total:      {len(train_df)+len(val_df)+len(test_df):>8,}\")\n\nprint(f\"\\n--- Category Distribution Across Splits ---\")\nfor split_name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n    dist = split_df['category'].value_counts(normalize=True)\n    print(f\"\\n{split_name}:\")\n    for cat in category_labels:\n        pct = dist.get(cat, 0) * 100\n        print(f\"  {cat:<20s}: {pct:5.1f}%\")\n\nprint(f\"\\n--- Severity Distribution Across Splits ---\")\nfor split_name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n    dist = split_df['severity_label'].value_counts(normalize=True)\n    print(f\"\\n{split_name}:\")\n    for sev in severity_labels:\n        pct = dist.get(sev, 0) * 100\n        print(f\"  {sev:<10s}: {pct:5.1f}%\")\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfor i, (name, sdf) in enumerate([('Train', train_df), ('Val', val_df), ('Test', test_df)]):\n    sdf['category'].value_counts().plot(kind='bar', ax=axes[0][i], color='steelblue')\n    axes[0][i].set_title(f'{name} - Categories')\n    axes[0][i].tick_params(axis='x', rotation=45)\n\n    sdf['severity_label'].value_counts().plot(kind='bar', ax=axes[1][i], color='coral')\n    axes[1][i].set_title(f'{name} - Severity')\n    axes[1][i].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.savefig('figures/03_split_distributions.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved: figures/03_split_distributions.png\")\n\ntrain_df.drop(columns=['strat_key']).to_csv('data/splits/train.csv', index=False)\nval_df.drop(columns=['strat_key']).to_csv('data/splits/val.csv', index=False)\ntest_df.drop(columns=['strat_key']).to_csv('data/splits/test.csv', index=False)\nprint(\"✓ Saved: data/splits/train.csv, val.csv, test.csv\")\n\n\n# ==============================================================================\n# SECTION 4: FORMATTING FOR FINE-TUNING (3 Points)\n# ==============================================================================\n\"\"\"\nFORMATTING FOR RoBERTa FINE-TUNING:\n- Tokenize with RoBERTa tokenizer (max_length=256)\n- Create PyTorch Dataset class with dual labels (category + severity)\n- Build DataLoaders with appropriate batch sizes\n- Compute class weights for weighted loss during training\n- Save tokenized datasets for reproducible training\n\"\"\"\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SECTION 4: FORMATTING FOR FINE-TUNING\")\nprint(\"=\" * 70)\n\nMODEL_NAME = \"roberta-base\"\nMAX_LENGTH = 256\nBATCH_SIZE = 16\n\nprint(f\"\\nModel: {MODEL_NAME}\")\nprint(f\"Max sequence length: {MAX_LENGTH}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(f\"✓ Loaded tokenizer: vocab size = {tokenizer.vocab_size:,}\")\n\n# ---------------------------------------------------------------------------\n# 4.1 TOKEN LENGTH ANALYSIS\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 4.1 Token Length Analysis ---\")\n\nsample_texts = train_df['clean_description'].sample(min(5000, len(train_df)), random_state=RANDOM_SEED)\ntoken_lengths = [len(tokenizer.encode(t, add_special_tokens=True)) for t in tqdm(sample_texts, desc=\"Analyzing token lengths\")]\n\nprint(f\"Token length statistics:\")\nprint(f\"  Mean:   {np.mean(token_lengths):.1f}\")\nprint(f\"  Median: {np.median(token_lengths):.1f}\")\nprint(f\"  Std:    {np.std(token_lengths):.1f}\")\nprint(f\"  Min:    {np.min(token_lengths)}\")\nprint(f\"  Max:    {np.max(token_lengths)}\")\nprint(f\"  95th percentile: {np.percentile(token_lengths, 95):.0f}\")\nprint(f\"  99th percentile: {np.percentile(token_lengths, 99):.0f}\")\n\ncoverage = sum(1 for l in token_lengths if l <= MAX_LENGTH) / len(token_lengths) * 100\nprint(f\"\\n  Coverage at max_length={MAX_LENGTH}: {coverage:.1f}%\")\n\nplt.figure(figsize=(10, 5))\nplt.hist(token_lengths, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\nplt.axvline(x=MAX_LENGTH, color='red', linestyle='--', label=f'max_length={MAX_LENGTH}')\nplt.axvline(x=np.percentile(token_lengths, 95), color='orange', linestyle='--', label='95th percentile')\nplt.xlabel('Token Length')\nplt.ylabel('Count')\nplt.title('Distribution of Token Lengths (RoBERTa Tokenizer)')\nplt.legend()\nplt.tight_layout()\nplt.savefig('figures/04_token_lengths.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"✓ Saved: figures/04_token_lengths.png\")\n\n\n# ---------------------------------------------------------------------------\n# 4.2 PYTORCH DATASET CLASS\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 4.2 Creating PyTorch Dataset ---\")\n\nclass ViolationDataset(Dataset):\n    def __init__(self, texts, category_ids, severity_ids, tokenizer, max_length=256):\n        self.texts = texts.tolist()\n        self.category_ids = category_ids.tolist()\n        self.severity_ids = severity_ids.tolist()\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'category_label': torch.tensor(self.category_ids[idx], dtype=torch.long),\n            'severity_label': torch.tensor(self.severity_ids[idx], dtype=torch.long),\n        }\n\ntrain_dataset = ViolationDataset(\n    train_df['clean_description'], train_df['category_id'],\n    train_df['severity_id'], tokenizer, MAX_LENGTH\n)\nval_dataset = ViolationDataset(\n    val_df['clean_description'], val_df['category_id'],\n    val_df['severity_id'], tokenizer, MAX_LENGTH\n)\ntest_dataset = ViolationDataset(\n    test_df['clean_description'], test_df['category_id'],\n    test_df['severity_id'], tokenizer, MAX_LENGTH\n)\n\nprint(f\"Train dataset: {len(train_dataset):,} samples\")\nprint(f\"Val dataset:   {len(val_dataset):,} samples\")\nprint(f\"Test dataset:  {len(test_dataset):,} samples\")\n\nsample = train_dataset[0]\nprint(f\"\\nSample item shapes:\")\nprint(f\"  input_ids:      {sample['input_ids'].shape}\")\nprint(f\"  attention_mask:  {sample['attention_mask'].shape}\")\nprint(f\"  category_label:  {sample['category_label']} ({id2cat[sample['category_label'].item()]})\")\nprint(f\"  severity_label:  {sample['severity_label']} ({id2sev[sample['severity_label'].item()]})\")\n\n\n# ---------------------------------------------------------------------------\n# 4.3 DATALOADERS\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 4.3 Creating DataLoaders ---\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Train batches: {len(train_loader):,}\")\nprint(f\"Val batches:   {len(val_loader):,}\")\nprint(f\"Test batches:  {len(test_loader):,}\")\n\nbatch = next(iter(train_loader))\nprint(f\"\\nSample batch shapes:\")\nfor k, v in batch.items():\n    print(f\"  {k}: {v.shape}\")\n\n\n# ---------------------------------------------------------------------------\n# 4.4 CLASS WEIGHTS FOR IMBALANCED DATA\n# ---------------------------------------------------------------------------\n\nprint(\"\\n--- 4.4 Computing Class Weights ---\")\n\ncat_counts = train_df['category_id'].value_counts().sort_index()\ncat_weights = len(train_df) / (len(cat_counts) * cat_counts.values)\ncat_weights_tensor = torch.FloatTensor(cat_weights)\n\nsev_counts = train_df['severity_id'].value_counts().sort_index()\nsev_weights = len(train_df) / (len(sev_counts) * sev_counts.values)\nsev_weights_tensor = torch.FloatTensor(sev_weights)\n\nprint(f\"\\nCategory class weights:\")\nfor i, w in enumerate(cat_weights_tensor):\n    print(f\"  {id2cat[i]:<20s}: {w:.4f}\")\n\nprint(f\"\\nSeverity class weights:\")\nfor i, w in enumerate(sev_weights_tensor):\n    print(f\"  {id2sev[i]:<10s}: {w:.4f}\")\n\ntorch.save({\n    'train_dataset': train_dataset,\n    'val_dataset': val_dataset,\n    'test_dataset': test_dataset,\n    'cat_weights': cat_weights_tensor,\n    'sev_weights': sev_weights_tensor,\n    'label_maps': label_maps,\n    'config': {\n        'model_name': MODEL_NAME,\n        'max_length': MAX_LENGTH,\n        'batch_size': BATCH_SIZE,\n        'num_categories': len(cat2id),\n        'num_severities': len(sev2id),\n        'random_seed': RANDOM_SEED,\n    }\n}, 'data/processed/tokenized_datasets.pt')\nprint(\"\\n✓ Saved: data/processed/tokenized_datasets.pt\")\n\n\n# ==============================================================================\n# FINAL SUMMARY\n# ==============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DATASET PREPARATION COMPLETE — SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\"\"\n┌────────────────────────────────────────────────────────────────┐\n│                    PREPARATION SUMMARY                         │\n├────────────────────────────────────────────────────────────────┤\n│ Raw records loaded:          {len(raw_df):>8,}                         │\n│ After cleaning & mapping:    {len(df_processed):>8,}                         │\n│ After deduplication:         {len(df_processed):>8,}                         │\n│                                                                │\n│ Train set:                   {len(train_df):>8,}  (80%)                    │\n│ Validation set:              {len(val_df):>8,}  (10%)                    │\n│ Test set:                    {len(test_df):>8,}  (10%)                    │\n│                                                                │\n│ Violation categories:        {len(cat2id):>8}                            │\n│ Severity levels:             {len(sev2id):>8}                            │\n│ Max token length:            {MAX_LENGTH:>8}                            │\n│ Tokenizer:                   {MODEL_NAME:>15}                     │\n│                                                                │\n│ Files saved:                                                   │\n│   data/processed/violations_cleaned.csv                        │\n│   data/processed/label_maps.json                               │\n│   data/processed/tokenized_datasets.pt                         │\n│   data/splits/train.csv                                        │\n│   data/splits/val.csv                                          │\n│   data/splits/test.csv                                         │\n│   figures/01_raw_distributions.png                              │\n│   figures/02_cleaned_distributions.png                          │\n│   figures/03_split_distributions.png                            │\n│   figures/04_token_lengths.png                                  │\n└────────────────────────────────────────────────────────────────┘\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:45:13.217785Z","iopub.execute_input":"2026-02-07T15:45:13.218607Z","iopub.status.idle":"2026-02-07T15:45:49.777997Z","shell.execute_reply.started":"2026-02-07T15:45:13.218578Z","shell.execute_reply":"2026-02-07T15:45:49.777287Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nDATASET PREPARATION FOR BUILDING CODE VIOLATION DETECTION\n======================================================================\n\n======================================================================\nSECTION 1: DATASET SELECTION\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/1321845432.py:34: DtypeWarning: Columns (23,28,30,32,33,34,35,36,37,38,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n  raw_df = pd.read_csv(\"data/raw/ecb_violations.csv\")\n","output_type":"stream"},{"name":"stdout","text":"\n✓ Loaded raw dataset: 300,000 records, 42 columns\n\n--- Raw Dataset Overview ---\nShape: (300000, 42)\n\n\nRaw Violation Type Distribution:\nviolation_type\nConstruction           156931\nElevators               52893\nUnknown                 40800\nBoilers                 14402\nLocal Law                7960\nSigns                    6699\nPublic Assembly          5783\nSite Safety              3713\nZoning                   3169\nCranes and Derricks      3024\nPlumbing                 2839\nQuality of Life          1494\nHPD                       278\nAdministrative             15\nName: count, dtype: int64\n\nRaw Severity Distribution:\nseverity\nCLASS - 2        100380\nNon-Hazardous     93596\nCLASS - 1         71057\nHazardous         17151\nCLASS - 3         14983\nUnknown            2833\nName: count, dtype: int64\n✓ Saved: figures/01_raw_distributions.png\n\n======================================================================\nSECTION 2: PREPROCESSING AND DATA CLEANING\n======================================================================\n\n--- 2.1 Handling Missing Values ---\nBefore: 300,000 records\n\nMissing values in key columns:\nviolation_type             0\nviolation_description    713\nseverity                   0\ndtype: int64\nAfter dropping nulls + short descriptions: 292,526 records\nDropped: 7,474 records (2.5%)\n\n--- 2.2 Cleaning Violation Descriptions ---\nCleaning violation descriptions...\n\nCleaning Statistics:\n  Total processed: 292,526\n  Successfully cleaned: 292,516\n  Empty after cleaning: 10\nRecords after cleaning: 292,501\n\n--- Before vs After Cleaning Examples ---\n\nExample 1:\n  BEFORE: FAILURE TO PROVIDE AT LEAST TWO MEANS OF EGRESS FROM OR SPACE WHERE REQUIRED. NOTE: DEPARTMENT OF BUILDINGS RECORDS INDI...\n  AFTER:  FAILURE TO PROVIDE AT LEAST TWO MEANS OF EGRESS FROM OR SPACE WHERE REQUIRED. NOTE: DEPARTMENT OF BUILDINGS RECORDS INDI...\n\nExample 2:\n  BEFORE: FAILURE TO MAINTAIN BUILDING HAZARDOUS. NOTE: AT THE CELLAR LEVEL THE BOILER ROOM CEILING IS EXPOSE TO WOOD JOINS AND AB...\n  AFTER:  FAILURE TO MAINTAIN BUILDING HAZARDOUS. NOTE: AT THE <FLOOR> LEVEL THE BOILER ROOM CEILING IS EXPOSE TO WOOD JOINS AND A...\n\nExample 3:\n  BEFORE: FAILURE TO SAFEGUARD PUBLIC & PROPERTY AFFECTED BY DEMO OPERATIONS. UPON INSP AT 325 20TH ST 2 STORIES BLDG IS DOWN, SIT...\n  AFTER:  FAILURE TO SAFEGUARD PUBLIC & PROPERTY AFFECTED BY DEMO OPERATIONS. UPON INSP AT <ADDR> 2 STORIES BLDG IS DOWN, SITE IS ...\n\nExample 4:\n  BEFORE: MECH DEMO W/O PERMIT. UPON INSPECTION AT 189 3RD AVE DEMO CREW ON SITEREMOVING 1 STORIES BUILDING REAR WITH MACHINES NO ...\n  AFTER:  MECH DEMO W/O PERMIT. UPON INSPECTION AT <ADDR> DEMO CREW ON SITEREMOVING 1 STORIES BUILDING REAR WITH MACHINES NO PERMI...\n\nExample 5:\n  BEFORE: MECH DEMO W/O PERMIT UPON INSP AT 294 BUTLER ST DEMO CREW ON SITE USING MACHINE TO REMOVE BUILDING REAR NO PERMIT WAS IS...\n  AFTER:  MECH DEMO W/O PERMIT UPON INSP AT <ADDR> DEMO CREW ON SITE USING MACHINE TO REMOVE BUILDING REAR NO PERMIT WAS ISSUED FO...\n\n--- 2.3 Label Mapping: Violation Categories ---\nRecords after removing 'Unknown' type: 251,704\n\nMapped Category Distribution:\ncategory\nConstruction       157175\nElevators           48289\nRegulatory          20434\nMechanical          11578\nSite Safety          6731\nZoning               3167\nPlumbing             2836\nQuality of Life      1494\nName: count, dtype: int64\n\nTotal categories: 8\n\n--- 2.4 Label Mapping: Severity ---\nRecords after removing 'Unknown' severity: 249,037\n\nMapped Severity Distribution:\nseverity_label\nLOW       100137\nMEDIUM     76240\nHIGH       72660\nName: count, dtype: int64\n\n--- 2.5 Encoding Labels ---\n\nCategory Label Encoding:\n  0: Construction         (155,595 samples)\n  1: Elevators            (48,287 samples)\n  2: Mechanical           (11,396 samples)\n  3: Plumbing             (2,783 samples)\n  4: Quality of Life      (1,492 samples)\n  5: Regulatory           (19,630 samples)\n  6: Site Safety          (6,695 samples)\n  7: Zoning               (3,159 samples)\n\nSeverity Label Encoding:\n  0: LOW        (100,137 samples)\n  1: MEDIUM     (76,240 samples)\n  2: HIGH       (72,660 samples)\n✓ Saved: data/processed/label_maps.json\n\n--- 2.6 Deduplication & Validation ---\nBefore dedup: 249,037\nAfter dedup: 236,445\n✓ All validation checks passed\n\n--- 2.7 Class Imbalance Analysis ---\n✓ Saved: figures/02_cleaned_distributions.png\n\nCategory imbalance ratio (max/min): 104.3x\nSeverity imbalance ratio (max/min): 1.4x\n→ Will use class weights during training to handle imbalance\n\n✓ Saved: data/processed/violations_cleaned.csv (236,445 records)\n\n======================================================================\nSECTION 3: TRAIN / VALIDATION / TEST SPLITTING\n======================================================================\nSmallest stratification group: 9 samples\n\n--- Split Sizes ---\nTrain:       189,156 (80.0%)\nValidation:   23,644 (10.0%)\nTest:         23,645 (10.0%)\nTotal:       236,445\n\n--- Category Distribution Across Splits ---\n\nTrain:\n  Construction        :  63.6%\n  Elevators           :  19.8%\n  Mechanical          :   4.6%\n  Plumbing            :   1.2%\n  Quality of Life     :   0.6%\n  Regulatory          :   6.1%\n  Site Safety         :   2.8%\n  Zoning              :   1.3%\n\nVal:\n  Construction        :  63.6%\n  Elevators           :  19.8%\n  Mechanical          :   4.6%\n  Plumbing            :   1.2%\n  Quality of Life     :   0.6%\n  Regulatory          :   6.1%\n  Site Safety         :   2.8%\n  Zoning              :   1.3%\n\nTest:\n  Construction        :  63.6%\n  Elevators           :  19.8%\n  Mechanical          :   4.6%\n  Plumbing            :   1.2%\n  Quality of Life     :   0.6%\n  Regulatory          :   6.1%\n  Site Safety         :   2.8%\n  Zoning              :   1.3%\n\n--- Severity Distribution Across Splits ---\n\nTrain:\n  LOW       :  40.8%\n  MEDIUM    :  29.4%\n  HIGH      :  29.8%\n\nVal:\n  LOW       :  40.8%\n  MEDIUM    :  29.4%\n  HIGH      :  29.8%\n\nTest:\n  LOW       :  40.8%\n  MEDIUM    :  29.4%\n  HIGH      :  29.8%\n✓ Saved: figures/03_split_distributions.png\n✓ Saved: data/splits/train.csv, val.csv, test.csv\n\n======================================================================\nSECTION 4: FORMATTING FOR FINE-TUNING\n======================================================================\n\nModel: roberta-base\nMax sequence length: 256\nBatch size: 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3cc8a0a49f4b92a94ffe156ca37429"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ebb667b0504765acf39dc4fb73a4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d35b0f8ca914864b87ebb248329f8c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"414ed344003d4a569b42f4fa421a4987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4fea372b4214e0db98af1a063cc4588"}},"metadata":{}},{"name":"stdout","text":"✓ Loaded tokenizer: vocab size = 50,265\n\n--- 4.1 Token Length Analysis ---\n","output_type":"stream"},{"name":"stderr","text":"Analyzing token lengths: 100%|██████████| 5000/5000 [00:00<00:00, 7112.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Token length statistics:\n  Mean:   63.4\n  Median: 70.0\n  Std:    21.1\n  Min:    7\n  Max:    134\n  95th percentile: 88\n  99th percentile: 102\n\n  Coverage at max_length=256: 100.0%\n✓ Saved: figures/04_token_lengths.png\n\n--- 4.2 Creating PyTorch Dataset ---\nTrain dataset: 189,156 samples\nVal dataset:   23,644 samples\nTest dataset:  23,645 samples\n\nSample item shapes:\n  input_ids:      torch.Size([256])\n  attention_mask:  torch.Size([256])\n  category_label:  0 (Construction)\n  severity_label:  1 (MEDIUM)\n\n--- 4.3 Creating DataLoaders ---\nTrain batches: 11,823\nVal batches:   1,478\nTest batches:  1,478\n\nSample batch shapes:\n  input_ids: torch.Size([16, 256])\n  attention_mask: torch.Size([16, 256])\n  category_label: torch.Size([16])\n  severity_label: torch.Size([16])\n\n--- 4.4 Computing Class Weights ---\n\nCategory class weights:\n  Construction        : 0.1964\n  Elevators           : 0.6327\n  Mechanical          : 2.7366\n  Plumbing            : 10.7037\n  Quality of Life     : 20.5069\n  Regulatory          : 2.0343\n  Site Safety         : 4.4646\n  Zoning              : 9.4882\n\nSeverity class weights:\n  LOW       : 0.8174\n  MEDIUM    : 1.1323\n  HIGH      : 1.1192\n\n✓ Saved: data/processed/tokenized_datasets.pt\n\n======================================================================\nDATASET PREPARATION COMPLETE — SUMMARY\n======================================================================\n\n┌────────────────────────────────────────────────────────────────┐\n│                    PREPARATION SUMMARY                         │\n├────────────────────────────────────────────────────────────────┤\n│ Raw records loaded:           300,000                         │\n│ After cleaning & mapping:     236,445                         │\n│ After deduplication:          236,445                         │\n│                                                                │\n│ Train set:                    189,156  (80%)                    │\n│ Validation set:                23,644  (10%)                    │\n│ Test set:                      23,645  (10%)                    │\n│                                                                │\n│ Violation categories:               8                            │\n│ Severity levels:                    3                            │\n│ Max token length:                 256                            │\n│ Tokenizer:                      roberta-base                     │\n│                                                                │\n│ Files saved:                                                   │\n│   data/processed/violations_cleaned.csv                        │\n│   data/processed/label_maps.json                               │\n│   data/processed/tokenized_datasets.pt                         │\n│   data/splits/train.csv                                        │\n│   data/splits/val.csv                                          │\n│   data/splits/test.csv                                         │\n│   figures/01_raw_distributions.png                              │\n│   figures/02_cleaned_distributions.png                          │\n│   figures/03_split_distributions.png                            │\n│   figures/04_token_lengths.png                                  │\n└────────────────────────────────────────────────────────────────┘\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport shutil\n\nos.makedirs('checkpoints', exist_ok=True)\nos.makedirs('data/processed', exist_ok=True)\n\n# Kaggle uploads go to /kaggle/input/ — find your files\n# List what's there:\nfor root, dirs, files in os.walk('/kaggle/input'):\n    for f in files:\n        print(os.path.join(root, f))\n        \nshutil.copy('/kaggle/input/checkpoints/final_model.pt', 'checkpoints/final_model.pt')\nshutil.copy('/kaggle/input/checkpoints/tokenized_datasets.pt', 'data/processed/tokenized_datasets.pt')\n\nprint(\"✓ Files moved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:48:56.926597Z","iopub.execute_input":"2026-02-07T15:48:56.927180Z","iopub.status.idle":"2026-02-07T15:49:03.322841Z","shell.execute_reply.started":"2026-02-07T15:48:56.927154Z","shell.execute_reply":"2026-02-07T15:49:03.322185Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/checkpoints/final_model.pt\n/kaggle/input/checkpoints/tokenized_datasets.pt\n✓ Files moved\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os, json, time, torch, torch.nn as nn, numpy as np, pandas as pd\nfrom torch.utils.data import DataLoader, Dataset as TorchDataset\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nimport matplotlib; matplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nfor d in ['results', 'figures']:\n    os.makedirs(d, exist_ok=True)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {DEVICE}\")\n\nclass ViolationDataset(TorchDataset):\n    def __init__(self, texts, category_ids, severity_ids, tokenizer, max_length=256):\n        self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n        self.category_ids = category_ids.tolist() if hasattr(category_ids, 'tolist') else category_ids\n        self.severity_ids = severity_ids.tolist() if hasattr(severity_ids, 'tolist') else severity_ids\n        self.tokenizer = tokenizer; self.max_length = max_length\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, idx):\n        enc = self.tokenizer(self.texts[idx], max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n        return {'input_ids': enc['input_ids'].squeeze(), 'attention_mask': enc['attention_mask'].squeeze(),\n                'category_label': torch.tensor(self.category_ids[idx], dtype=torch.long),\n                'severity_label': torch.tensor(self.severity_ids[idx], dtype=torch.long)}\n\nclass ViolationClassifier(nn.Module):\n    def __init__(self, model_name, num_categories, num_severities, dropout=0.3):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        h = self.encoder.config.hidden_size; self.dropout = nn.Dropout(dropout)\n        self.category_head = nn.Sequential(nn.Linear(h,256), nn.ReLU(), nn.Dropout(dropout), nn.Linear(256,num_categories))\n        self.severity_head = nn.Sequential(nn.Linear(h,128), nn.ReLU(), nn.Dropout(dropout), nn.Linear(128,num_severities))\n    def forward(self, input_ids, attention_mask):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls = self.dropout(out.last_hidden_state[:, 0, :])\n        return self.category_head(cls), self.severity_head(cls)\n\nprint(\"Loading model...\")\nckpt = torch.load('checkpoints/final_model.pt', map_location='cpu', weights_only=False)\nmodel_cfg = ckpt['model_config']; label_maps = ckpt['label_maps']\nid2cat = {int(k):v for k,v in label_maps['id2cat'].items()}\nid2sev = {int(k):v for k,v in label_maps['id2sev'].items()}\ncat_names = [id2cat[i] for i in range(len(id2cat))]\nsev_names = [id2sev[i] for i in range(len(id2sev))]\n\nmodel = ViolationClassifier(model_cfg['model_name'], model_cfg['num_categories'], model_cfg['num_severities'], model_cfg['dropout'])\nmodel.load_state_dict(ckpt['model_state_dict']); model.to(DEVICE); model.eval()\nprint(f\"✓ Model loaded\")\n\ndata = torch.load('data/processed/tokenized_datasets.pt', map_location='cpu', weights_only=False)\ntest_dataset = data['test_dataset']\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\ntest_df = pd.read_csv('data/splits/test.csv')\nprint(f\"✓ Test: {len(test_dataset):,} samples\")\n\n@torch.no_grad()\ndef predict_all(mdl, loader):\n    cp,cl,sp,sl,cpr,spr = [],[],[],[],[],[]\n    for b in tqdm(loader, desc=\"Predicting\"):\n        ids = b['input_ids'].to(DEVICE); mask = b['attention_mask'].to(DEVICE)\n        c_log, s_log = mdl(ids, mask)\n        cpr.append(torch.softmax(c_log,1).detach().cpu().numpy())\n        spr.append(torch.softmax(s_log,1).detach().cpu().numpy())\n        cp.extend(c_log.argmax(1).detach().cpu().numpy())\n        cl.extend(b['category_label'].detach().cpu().numpy())\n        sp.extend(s_log.argmax(1).detach().cpu().numpy())\n        sl.extend(b['severity_label'].detach().cpu().numpy())\n    return np.array(cp),np.array(cl),np.array(sp),np.array(sl),np.vstack(cpr),np.vstack(spr)\n\n# ==============================================================================\n# SECTION 1: TEST SET EVALUATION\n# ==============================================================================\nprint(\"\\n\" + \"=\"*70 + \"\\nSECTION 1: TEST SET EVALUATION\\n\" + \"=\"*70)\n\ncat_preds, cat_true, sev_preds, sev_true, cat_probs, sev_probs = predict_all(model, test_loader)\n\nprint(\"\\n--- Category Report ---\")\ncat_report = classification_report(cat_true, cat_preds, target_names=cat_names, digits=4, output_dict=True)\nprint(classification_report(cat_true, cat_preds, target_names=cat_names, digits=4))\ncat_macro_f1 = f1_score(cat_true, cat_preds, average='macro')\ncat_weighted_f1 = f1_score(cat_true, cat_preds, average='weighted')\ncat_acc = accuracy_score(cat_true, cat_preds)\n\nprint(\"\\n--- Severity Report ---\")\nsev_report = classification_report(sev_true, sev_preds, target_names=sev_names, digits=4, output_dict=True)\nprint(classification_report(sev_true, sev_preds, target_names=sev_names, digits=4))\nsev_macro_f1 = f1_score(sev_true, sev_preds, average='macro')\nsev_weighted_f1 = f1_score(sev_true, sev_preds, average='weighted')\nsev_acc = accuracy_score(sev_true, sev_preds)\n\nprint(f\"Category: F1={cat_macro_f1:.4f}, Acc={cat_acc:.4f}\")\nprint(f\"Severity: F1={sev_macro_f1:.4f}, Acc={sev_acc:.4f}\")\n\n# ==============================================================================\n# SECTION 2: BASELINE COMPARISON (hardcoded from training output)\n# ==============================================================================\nprint(\"\\n\" + \"=\"*70 + \"\\nSECTION 2: BASELINE vs FINE-TUNED\\n\" + \"=\"*70)\n\nb_cat_f1, b_sev_f1, b_cat_acc, b_sev_acc = 0.0046, 0.1530, 0.0161, 0.2978\n\ncomparison = {\n    'Metric': ['Category F1','Category Acc','Severity F1','Severity Acc','Combined F1'],\n    'Baseline': [b_cat_f1, b_cat_acc, b_sev_f1, b_sev_acc, (b_cat_f1+b_sev_f1)/2],\n    'Fine-Tuned': [cat_macro_f1, cat_acc, sev_macro_f1, sev_acc, (cat_macro_f1+sev_macro_f1)/2],\n}\ncomparison['Improvement'] = [ft-bl for ft,bl in zip(comparison['Fine-Tuned'], comparison['Baseline'])]\nprint(\"\\n\" + pd.DataFrame(comparison).to_string(index=False, float_format='%.4f'))\n\nfig, ax = plt.subplots(figsize=(10,6))\nx = np.arange(5); w = 0.35\nax.bar(x-w/2, comparison['Baseline'], w, label='Baseline', color='#d9534f')\nax.bar(x+w/2, comparison['Fine-Tuned'], w, label='Fine-Tuned', color='#5cb85c')\nax.set_xticks(x); ax.set_xticklabels(comparison['Metric'], rotation=20, ha='right')\nax.legend(); ax.set_ylim(0,1.05); ax.set_title('Baseline vs Fine-Tuned')\nplt.tight_layout(); plt.savefig('figures/06_baseline_vs_finetuned.png', dpi=150); plt.show()\nprint(\"✓ Saved: figures/06_baseline_vs_finetuned.png\")\n\n# ==============================================================================\n# SECTION 3: CONFUSION MATRICES\n# ==============================================================================\nprint(\"\\n\" + \"=\"*70 + \"\\nSECTION 3: CONFUSION MATRICES\\n\" + \"=\"*70)\n\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\ncat_cm = confusion_matrix(cat_true, cat_preds)\ncat_cm_n = cat_cm.astype('float') / cat_cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cat_cm_n, annot=True, fmt='.2f', cmap='Blues', xticklabels=cat_names, yticklabels=cat_names, ax=axes[0])\naxes[0].set_title('Category Confusion Matrix'); axes[0].set_ylabel('True'); axes[0].set_xlabel('Predicted')\n\nsev_cm = confusion_matrix(sev_true, sev_preds)\nsev_cm_n = sev_cm.astype('float') / sev_cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(sev_cm_n, annot=True, fmt='.2f', cmap='Oranges', xticklabels=sev_names, yticklabels=sev_names, ax=axes[1])\naxes[1].set_title('Severity Confusion Matrix'); axes[1].set_ylabel('True'); axes[1].set_xlabel('Predicted')\nplt.tight_layout(); plt.savefig('figures/07_confusion_matrices.png', dpi=150); plt.show()\nprint(\"✓ Saved: figures/07_confusion_matrices.png\")\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\ncat_f1s = [cat_report[c]['f1-score'] for c in cat_names]\naxes[0].barh(cat_names, cat_f1s, color=['#d9534f' if f<0.7 else '#f0ad4e' if f<0.85 else '#5cb85c' for f in cat_f1s])\naxes[0].set_xlim(0,1); axes[0].set_title('Per-Class F1 — Category')\nfor i,v in enumerate(cat_f1s): axes[0].text(v+0.01, i, f'{v:.3f}', va='center')\n\nsev_f1s = [sev_report[s]['f1-score'] for s in sev_names]\naxes[1].barh(sev_names, sev_f1s, color=['#d9534f' if f<0.7 else '#f0ad4e' if f<0.85 else '#5cb85c' for f in sev_f1s])\naxes[1].set_xlim(0,1); axes[1].set_title('Per-Class F1 — Severity')\nfor i,v in enumerate(sev_f1s): axes[1].text(v+0.01, i, f'{v:.3f}', va='center')\nplt.tight_layout(); plt.savefig('figures/08_per_class_f1.png', dpi=150); plt.show()\nprint(\"✓ Saved: figures/08_per_class_f1.png\")\n\n# ==============================================================================\n# SECTION 4: ERROR ANALYSIS\n# ==============================================================================\nprint(\"\\n\" + \"=\"*70 + \"\\nSECTION 4: ERROR ANALYSIS\\n\" + \"=\"*70)\n\ntest_df = test_df.head(len(cat_preds)).copy()\ntest_df['cat_pred'] = [id2cat[p] for p in cat_preds]\ntest_df['cat_true'] = [id2cat[t] for t in cat_true]\ntest_df['sev_pred'] = [id2sev[p] for p in sev_preds]\ntest_df['sev_true'] = [id2sev[t] for t in sev_true]\ntest_df['cat_correct'] = test_df['cat_pred'] == test_df['cat_true']\ntest_df['sev_correct'] = test_df['sev_pred'] == test_df['sev_true']\ntest_df['cat_confidence'] = [cat_probs[i, cat_preds[i]] for i in range(len(cat_preds))]\ntest_df['sev_confidence'] = [sev_probs[i, sev_preds[i]] for i in range(len(sev_preds))]\n\ncat_errors = test_df[~test_df['cat_correct']]\nsev_errors = test_df[~test_df['sev_correct']]\n\nprint(f\"Category errors: {len(cat_errors):,}/{len(test_df):,} ({len(cat_errors)/len(test_df)*100:.1f}%)\")\nprint(f\"Severity errors: {len(sev_errors):,}/{len(test_df):,} ({len(sev_errors)/len(test_df)*100:.1f}%)\")\n\nprint(\"\\n--- Top Category Confusions ---\")\ncat_conf_pairs = cat_errors.groupby(['cat_true','cat_pred']).size().sort_values(ascending=False).head(10)\nprint(cat_conf_pairs.to_string())\n\nprint(\"\\n--- Top Severity Confusions ---\")\nsev_conf_pairs = sev_errors.groupby(['sev_true','sev_pred']).size().sort_values(ascending=False).head(10)\nprint(sev_conf_pairs.to_string())\n\nprint(\"\\n--- Silent Failures (High Confidence + Wrong) ---\")\nhc_cat = cat_errors[cat_errors['cat_confidence']>0.9].sort_values('cat_confidence', ascending=False)\nprint(f\"High-conf category errors (>90%): {len(hc_cat)}\")\nfor _, r in hc_cat.head(3).iterrows():\n    print(f\"  {str(r['clean_description'])[:80]}... TRUE:{r['cat_true']} → PRED:{r['cat_pred']} ({r['cat_confidence']:.3f})\")\n\ncritical = sev_errors[(sev_errors['sev_true']=='HIGH')&(sev_errors['sev_pred']=='LOW')]\nfalse_alarms = sev_errors[(sev_errors['sev_true']=='LOW')&(sev_errors['sev_pred']=='HIGH')]\nprint(f\"\\n⚠️ CRITICAL (HIGH→LOW): {len(critical)}\")\nfor _, r in critical.head(3).iterrows():\n    print(f\"  {str(r['clean_description'])[:80]}... conf:{r['sev_confidence']:.3f}\")\nprint(f\"⚠️ FALSE ALARMS (LOW→HIGH): {len(false_alarms)}\")\n\nprint(\"\\n--- Error Rate by Text Length ---\")\ntest_df['text_length'] = test_df['clean_description'].str.len()\ntest_df['length_bin'] = pd.cut(test_df['text_length'], bins=[0,100,200,300,500,10000], labels=['<100','100-200','200-300','300-500','500+'])\nprint(test_df.groupby('length_bin').agg(\n    total=('cat_correct','count'),\n    cat_err=('cat_correct', lambda x: 1-x.mean()),\n    sev_err=('sev_correct', lambda x: 1-x.mean())\n).round(4).to_string())\n\nweakest_cat = min(cat_names, key=lambda c: cat_report[c]['f1-score'])\nweakest_sev = min(sev_names, key=lambda s: sev_report[s]['f1-score'])\nprint(f\"\\n--- Suggested Improvements ---\")\nprint(f\"1. Weakest category: '{weakest_cat}' (F1:{cat_report[weakest_cat]['f1-score']:.4f}) → more data / augmentation\")\nprint(f\"2. Weakest severity: '{weakest_sev}' (F1:{sev_report[weakest_sev]['f1-score']:.4f}) → re-examine label mapping\")\nprint(f\"3. {len(critical)} critical safety misses → asymmetric loss penalizing HIGH→LOW\")\nprint(f\"4. {len(hc_cat)} high-confidence errors → temperature scaling for calibration\")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\ncat_conf_pairs.head(8).plot(kind='barh', ax=axes[0], color='#d9534f')\naxes[0].set_title('Top Confusions'); axes[0].set_xlabel('Count')\n\nconf_bins = [0, 0.3, 0.5, 0.7, 0.9, 1.0]\ntest_df['conf_bin'] = pd.cut(test_df['cat_confidence'], bins=conf_bins)\nce = test_df.groupby('conf_bin')['cat_correct'].agg(['mean','count'])\naxes[1].bar(range(len(ce)), 1-ce['mean'].values, color='#f0ad4e')\naxes[1].set_xticks(range(len(ce))); axes[1].set_xticklabels(['0-0.3','0.3-0.5','0.5-0.7','0.7-0.9','0.9-1.0'])\naxes[1].set_title('Error Rate by Confidence'); axes[1].set_ylabel('Error Rate')\nplt.tight_layout(); plt.savefig('figures/09_error_analysis.png', dpi=150); plt.show()\nprint(\"✓ Saved: figures/09_error_analysis.png\")\n\n# ==============================================================================\n# SECTION 5: INFERENCE PIPELINE\n# ==============================================================================\nprint(\"\\n\" + \"=\"*70 + \"\\nSECTION 5: INFERENCE PIPELINE\\n\" + \"=\"*70)\n\nclass ViolationInferencePipeline:\n    def __init__(self, model, tokenizer, id2cat, id2sev, max_length=256, device='cpu'):\n        self.model = model.to(device); self.model.eval()\n        self.tokenizer = tokenizer; self.id2cat = id2cat; self.id2sev = id2sev\n        self.max_length = max_length; self.device = device\n\n    @torch.no_grad()\n    def predict(self, text):\n        enc = self.tokenizer(text.upper(), max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n        ids = enc['input_ids'].to(self.device); mask = enc['attention_mask'].to(self.device)\n        c_log, s_log = self.model(ids, mask)\n        cp = torch.softmax(c_log,1).detach().cpu().numpy()[0]\n        sp = torch.softmax(s_log,1).detach().cpu().numpy()[0]\n        return {'category': self.id2cat[cp.argmax()], 'cat_conf': float(cp.max()),\n                'severity': self.id2sev[sp.argmax()], 'sev_conf': float(sp.max())}\n\n    @torch.no_grad()\n    def predict_batch(self, texts, batch_size=32):\n        results = []\n        for i in range(0, len(texts), batch_size):\n            bt = texts[i:i+batch_size]\n            enc = self.tokenizer([t.upper() for t in bt], max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n            ids = enc['input_ids'].to(self.device); mask = enc['attention_mask'].to(self.device)\n            c_log, s_log = self.model(ids, mask)\n            cp = torch.softmax(c_log,1).detach().cpu().numpy()\n            sp = torch.softmax(s_log,1).detach().cpu().numpy()\n            for j in range(len(bt)):\n                results.append({'text': bt[j][:80], 'category': self.id2cat[cp[j].argmax()],\n                                'cat_conf': float(cp[j].max()), 'severity': self.id2sev[sp[j].argmax()],\n                                'sev_conf': float(sp[j].max())})\n        return results\n\ntokenizer = AutoTokenizer.from_pretrained(model_cfg['model_name'])\npipeline = ViolationInferencePipeline(model, tokenizer, id2cat, id2sev, device=DEVICE)\n\ndemo_texts = [\n    \"FAILURE TO MAINTAIN BUILDING WALL NOTED BRICKS FALLING FROM FACADE POSING DANGER TO PEDESTRIANS\",\n    \"WORK WITHOUT A PERMIT CONTRACTOR PERFORMING ELECTRICAL WORK ON 3RD FLOOR WITHOUT DOB APPROVAL\",\n    \"ELEVATOR INSPECTION OVERDUE CERTIFICATE EXPIRED LAST YEAR BUILDING HAS 6 PASSENGER ELEVATORS\",\n    \"FENCE EXCEEDS PERMITTED HEIGHT IN FRONT YARD SETBACK AREA ZONING VIOLATION\",\n    \"FAILURE TO PROVIDE SITE SAFETY MANAGER DURING ACTIVE DEMOLITION OF 5 STORY BUILDING\",\n    \"BOILER FAILED ANNUAL INSPECTION DUE TO CRACKED HEAT EXCHANGER AND GAS LEAK DETECTED\",\n    \"ILLEGAL CONVERSION OF COMMERCIAL SPACE TO RESIDENTIAL USE WITHOUT CERTIFICATE OF OCCUPANCY\",\n    \"EXIT DOOR NOT SELF CLOSING ON 2ND FLOOR OF PUBLIC ASSEMBLY SPACE CAPACITY 300 PERSONS\",\n]\n\nprint(\"\\n--- Inference Demo ---\")\nfor t in demo_texts:\n    r = pipeline.predict(t)\n    print(f\"\\nInput:    {t[:80]}...\")\n    print(f\"Category: {r['category']} ({r['cat_conf']:.3f}) | Severity: {r['severity']} ({r['sev_conf']:.3f})\")\n\nprint(\"\\n--- Batch Performance ---\")\nsample = test_df['clean_description'].head(500).tolist()\nstart = time.time()\n_ = pipeline.predict_batch(sample, batch_size=64)\nelapsed = time.time() - start\nprint(f\"{len(sample)} samples in {elapsed:.2f}s | {len(sample)/elapsed:.0f} samples/sec | {elapsed/len(sample)*1000:.1f}ms/sample\")\n\n# ==============================================================================\n# SAVE RESULTS\n# ==============================================================================\nresults = {\n    'test_metrics': {'cat_f1': cat_macro_f1, 'cat_weighted_f1': cat_weighted_f1, 'cat_acc': cat_acc,\n                     'sev_f1': sev_macro_f1, 'sev_weighted_f1': sev_weighted_f1, 'sev_acc': sev_acc,\n                     'combined_f1': (cat_macro_f1+sev_macro_f1)/2},\n    'baseline': {'cat_f1': b_cat_f1, 'sev_f1': b_sev_f1},\n    'errors': {'cat_error_rate': len(cat_errors)/len(test_df), 'sev_error_rate': len(sev_errors)/len(test_df),\n               'critical_high_as_low': int(len(critical)), 'false_alarms': int(len(false_alarms)),\n               'high_conf_errors': int(len(hc_cat))},\n    'inference': {'throughput': len(sample)/elapsed, 'latency_ms': elapsed/len(sample)*1000}\n}\nwith open('results/evaluation_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\nprint(\"\\n✓ Saved: results/evaluation_results.json\")\n\n# ==============================================================================\n# FINAL SUMMARY\n# ==============================================================================\nprint(f\"\"\"\n{'='*70}\nEVALUATION COMPLETE\n{'='*70}\n\n  TEST SET:  Cat F1={cat_macro_f1:.4f}  Sev F1={sev_macro_f1:.4f}  Combined={((cat_macro_f1+sev_macro_f1)/2):.4f}\n  BASELINE:  Cat F1={b_cat_f1:.4f}  Sev F1={b_sev_f1:.4f}\n  IMPROVEMENT: Cat +{cat_macro_f1-b_cat_f1:.4f}  Sev +{sev_macro_f1-b_sev_f1:.4f}\n  ERRORS: {len(critical)} critical (HIGH→LOW) | {len(false_alarms)} false alarms | {len(hc_cat)} high-conf errors\n  INFERENCE: {len(sample)/elapsed:.0f} samples/sec | {elapsed/len(sample)*1000:.1f}ms/sample\n\n  Files: figures/06-09, results/evaluation_results.json\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T15:49:09.757910Z","iopub.execute_input":"2026-02-07T15:49:09.758214Z","iopub.status.idle":"2026-02-07T15:55:46.313461Z","shell.execute_reply.started":"2026-02-07T15:49:09.758182Z","shell.execute_reply":"2026-02-07T15:55:46.312627Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nLoading model...\n","output_type":"stream"},{"name":"stderr","text":"2026-02-07 15:49:12.633712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770479352.790930      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770479352.837467      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770479353.220450      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770479353.220498      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770479353.220501      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770479353.220504      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b411f259f9c4b8eb66c89a9f0ed60f0"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"✓ Model loaded\n✓ Test: 23,645 samples\n\n======================================================================\nSECTION 1: TEST SET EVALUATION\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Predicting:   0%|          | 0/370 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPredicting: 100%|██████████| 370/370 [06:07<00:00,  1.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Category Report ---\n                 precision    recall  f1-score   support\n\n   Construction     0.9955    0.9664    0.9807     15048\n      Elevators     0.9443    0.9876    0.9655      4673\n     Mechanical     0.9339    0.7593    0.8376      1080\n       Plumbing     0.8713    0.9600    0.9135       275\nQuality of Life     0.7150    0.9583    0.8190       144\n     Regulatory     0.9228    0.9876    0.9541      1453\n    Site Safety     0.7416    0.9682    0.8399       661\n         Zoning     0.8151    0.9357    0.8713       311\n\n       accuracy                         0.9620     23645\n      macro avg     0.8675    0.9404    0.8977     23645\n   weighted avg     0.9655    0.9620    0.9624     23645\n\n\n--- Severity Report ---\n              precision    recall  f1-score   support\n\n         LOW     0.9230    0.8979    0.9103      9643\n      MEDIUM     0.8277    0.8600    0.8436      6959\n        HIGH     0.8378    0.8367    0.8373      7043\n\n    accuracy                         0.8685     23645\n   macro avg     0.8628    0.8649    0.8637     23645\nweighted avg     0.8696    0.8685    0.8689     23645\n\nCategory: F1=0.8977, Acc=0.9620\nSeverity: F1=0.8637, Acc=0.8685\n\n======================================================================\nSECTION 2: BASELINE vs FINE-TUNED\n======================================================================\n\n      Metric  Baseline  Fine-Tuned  Improvement\n Category F1    0.0046      0.8977       0.8931\nCategory Acc    0.0161      0.9620       0.9459\n Severity F1    0.1530      0.8637       0.7107\nSeverity Acc    0.2978      0.8685       0.5707\n Combined F1    0.0788      0.8807       0.8019\n✓ Saved: figures/06_baseline_vs_finetuned.png\n\n======================================================================\nSECTION 3: CONFUSION MATRICES\n======================================================================\n✓ Saved: figures/07_confusion_matrices.png\n✓ Saved: figures/08_per_class_f1.png\n\n======================================================================\nSECTION 4: ERROR ANALYSIS\n======================================================================\nCategory errors: 899/23,645 (3.8%)\nSeverity errors: 3,109/23,645 (13.1%)\n\n--- Top Category Confusions ---\ncat_true      cat_pred       \nMechanical    Elevators          254\nConstruction  Site Safety        220\n              Regulatory         109\n              Zoning              64\n              Quality of Life     55\nElevators     Mechanical          51\nConstruction  Plumbing            35\n              Elevators           16\nSite Safety   Construction        16\nZoning        Construction        16\n\n--- Top Severity Confusions ---\nsev_true  sev_pred\nMEDIUM    HIGH        727\nHIGH      MEDIUM      675\nLOW       MEDIUM      571\nHIGH      LOW         475\nLOW       HIGH        414\nMEDIUM    LOW         247\n\n--- Silent Failures (High Confidence + Wrong) ---\nHigh-conf category errors (>90%): 370\n  FAILURE TO NOTIFY THIS DEPT. IMMEDIATELY AFTER AN ACCIDENT CONCERNING PUBLIC OR ... TRUE:Construction → PRED:Site Safety (0.999)\n  FAILURE TO MAINTAIN BLDG IN CODE-COMPLIANT MANNER. PVC DRAIN PIPING INSTALLED IN... TRUE:Construction → PRED:Plumbing (0.999)\n  FAILURE TO HAVE APPROVED P.A. PLANS AVAILBLE FOR INSPECTION. EXIT LIGHTING INOPE... TRUE:Construction → PRED:Regulatory (0.999)\n\n⚠️ CRITICAL (HIGH→LOW): 475\n  41X. 41 RECONNECT WIRES.... conf:0.995\n  WORK DOES NOT CONFORM TO APPROVED CONSTRUCTION DOCUMENTS FOR THIS PRO CERT ALT <... conf:0.623\n  FAILURE TO MAINTAIN DEFECT IS BROKEN AND DEFECTIVE COMCRETE BALCONYS AT THE SOUT... conf:0.834\n⚠️ FALSE ALARMS (LOW→HIGH): 414\n\n--- Error Rate by Text Length ---\n            total  cat_err  sev_err\nlength_bin                         \n<100         4230   0.0700   0.0650\n100-200      9370   0.0307   0.1110\n200-300     10045   0.0314   0.1786\n300-500         0      NaN      NaN\n500+            0      NaN      NaN\n\n--- Suggested Improvements ---\n1. Weakest category: 'Quality of Life' (F1:0.8190) → more data / augmentation\n2. Weakest severity: 'HIGH' (F1:0.8373) → re-examine label mapping\n3. 475 critical safety misses → asymmetric loss penalizing HIGH→LOW\n4. 370 high-confidence errors → temperature scaling for calibration\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/997642835.py:197: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  print(test_df.groupby('length_bin').agg(\n/tmp/ipykernel_55/997642835.py:217: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  ce = test_df.groupby('conf_bin')['cat_correct'].agg(['mean','count'])\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved: figures/09_error_analysis.png\n\n======================================================================\nSECTION 5: INFERENCE PIPELINE\n======================================================================\n\n--- Inference Demo ---\n\nInput:    FAILURE TO MAINTAIN BUILDING WALL NOTED BRICKS FALLING FROM FACADE POSING DANGER...\nCategory: Construction (0.978) | Severity: HIGH (0.731)\n\nInput:    WORK WITHOUT A PERMIT CONTRACTOR PERFORMING ELECTRICAL WORK ON 3RD FLOOR WITHOUT...\nCategory: Construction (0.990) | Severity: LOW (0.909)\n\nInput:    ELEVATOR INSPECTION OVERDUE CERTIFICATE EXPIRED LAST YEAR BUILDING HAS 6 PASSENG...\nCategory: Elevators (0.428) | Severity: LOW (0.813)\n\nInput:    FENCE EXCEEDS PERMITTED HEIGHT IN FRONT YARD SETBACK AREA ZONING VIOLATION...\nCategory: Construction (0.919) | Severity: LOW (0.996)\n\nInput:    FAILURE TO PROVIDE SITE SAFETY MANAGER DURING ACTIVE DEMOLITION OF 5 STORY BUILD...\nCategory: Site Safety (0.966) | Severity: HIGH (0.992)\n\nInput:    BOILER FAILED ANNUAL INSPECTION DUE TO CRACKED HEAT EXCHANGER AND GAS LEAK DETEC...\nCategory: Mechanical (0.999) | Severity: LOW (0.533)\n\nInput:    ILLEGAL CONVERSION OF COMMERCIAL SPACE TO RESIDENTIAL USE WITHOUT CERTIFICATE OF...\nCategory: Zoning (0.994) | Severity: LOW (0.953)\n\nInput:    EXIT DOOR NOT SELF CLOSING ON 2ND FLOOR OF PUBLIC ASSEMBLY SPACE CAPACITY 300 PE...\nCategory: Regulatory (0.998) | Severity: LOW (0.974)\n\n--- Batch Performance ---\n500 samples in 8.18s | 61 samples/sec | 16.4ms/sample\n\n✓ Saved: results/evaluation_results.json\n\n======================================================================\nEVALUATION COMPLETE\n======================================================================\n\n  TEST SET:  Cat F1=0.8977  Sev F1=0.8637  Combined=0.8807\n  BASELINE:  Cat F1=0.0046  Sev F1=0.1530\n  IMPROVEMENT: Cat +0.8931  Sev +0.7107\n  ERRORS: 475 critical (HIGH→LOW) | 414 false alarms | 370 high-conf errors\n  INFERENCE: 61 samples/sec | 16.4ms/sample\n\n  Files: figures/06-09, results/evaluation_results.json\n\n","output_type":"stream"}],"execution_count":8}]}